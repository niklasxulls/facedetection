{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import *\n",
    "import numpy as np\n",
    "import copy\n",
    "from functools import partial\n",
    "from tfrecord_lite import decode_example\n",
    "import sys, os\n",
    "os.chdir('..')\n",
    "join = os.path.join(os.getcwd(), '_global')\n",
    "sys.path.extend([join])\n",
    "from _global.config import *\n",
    "from _global.funcs import *\n",
    "NUMERIC_COLUMS = ['x1', 'y1', 'w', 'h', 'blur']\n",
    "CATEGORICAL_COLUMS = ['blur', 'expression', 'illumniation',  'invalid', \n",
    "'occlusion', 'pose']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_description = {\n",
    "#     'filename': tf.io.VarLenFeature(tf.string),\n",
    "#     'shape' : tf.io.FixedLenFeature([3], tf.int64),\n",
    "#     'image' : tf.io.FixedLenFeature([], tf.string),\n",
    "#     'bbxs' : tf.io.VarLenFeature(tf.int64), \n",
    "#     'faces' : tf.io.FixedLenFeature([1], tf.int64),\n",
    "# }\n",
    "\n",
    "\n",
    "# def parse_function(example_proto):\n",
    "#   # Parse the input `tf.Example` proto using the dictionary above.\n",
    "#   example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "#   image, _ = decode_img(example['image'])\n",
    "#   image = tf.cast(image, tf.uint8)\n",
    "#   #   print(example['bbxs'])\n",
    "#   bbxs = tf.cast(example['bbxs'], tf.int64)\n",
    "\n",
    "#   if isinstance(bbxs, tf.SparseTensor):\n",
    "#       bbxs = tf.sparse.to_dense(bbxs)\n",
    "      \n",
    "#   return image, bbxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filenames = [OUTPUT_TRAIN_TFRECORD]\n",
    "train_dataset = prepare_dataset(train_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_filenames = [OUTPUT_VALIDATION_TFRECORD]\n",
    "validation_dataset = prepare_dataset(validation_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = applications.VGG19(\n",
    "    include_top=False,\n",
    "    input_shape=(*IMAGE_SIZES, 3),\n",
    "    weights='imagenet'\n",
    ")\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([base_model])\n",
    "\n",
    "model.add(layers.Conv2D(1024, (4, 4), activation='relu', padding=\"same\"))\n",
    "model.add(layers.MaxPool2D(strides=(2, 2), padding=\"same\"))\n",
    "model.add(layers.Conv2D(1024, (4,4), activation='relu', padding=\"same\"))\n",
    "model.add(layers.MaxPool2D(strides=(2, 2), padding=\"same\"))\n",
    "model.add(layers.Conv2D(1024, (4,4), activation='relu', padding=\"same\"))\n",
    "model.add(layers.MaxPool2D(strides=(2, 2), padding=\"same\"))\n",
    "model.add(layers.Conv2D(512, (4,4), activation='relu', padding=\"same\"))\n",
    "model.add(layers.MaxPool2D(strides=(2, 2), padding=\"same\"))\n",
    "model.add(layers.Conv2D(512, (4,4), activation='relu', padding=\"same\"))\n",
    "model.add(layers.MaxPool2D(strides=(2, 2), padding=\"same\"))\n",
    "model.add(layers.Conv2D(256, (4,4), activation='relu', padding=\"same\"))\n",
    "model.add(layers.MaxPool2D(strides=(2, 2), padding=\"same\"))\n",
    "model.add(layers.Conv2D(128, (4,4), activation='relu', padding=\"same\"))\n",
    "model.add(layers.MaxPool2D(strides=(2, 2), padding=\"same\"))\n",
    "model.add(layers.Conv2D(64, (4,4), activation='relu', padding=\"same\"))\n",
    "model.add(layers.MaxPool2D(strides=(2, 2), padding=\"same\"))\n",
    "# model.add(layers.Conv2D(512, (3,3), activation='relu', padding=\"same\"))\n",
    "# model.add(layers.MaxPool2D(strides=(2, 2)))\n",
    "# model.add(layers.Conv2D(256, (3,3), activation='relu', input_shape=(1, 1, 256), data_format='channels_first'))\n",
    "# model.add(layers.MaxPool2D((2, 2)))\n",
    "# model.add(layers.Conv2D(512, (3,3), activation='relu'))\n",
    "# model.add(layers.MaxPool2D((2, 2)))\n",
    "# model.add(tf.keras.layers.Flatten())\n",
    "# model.add(tf.keras.layers.Dense(4096, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(4096, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(4096, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(4096, activation='relu'))\n",
    "# # 10 because its amount of classes we have\n",
    "model.add(layers.Dense(MAX_BBXS * 4, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(RAW_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl = tfa.losses.GIoULoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_practical(y_true, y_pred):\n",
    "        # now contains 32 lists (a batch) of bbxs -> shape is (32, 7876)\n",
    "        # print(y_true)\n",
    "        batch_size = y_pred.shape[0]\n",
    "        bbx_true = y_true\n",
    "\n",
    "        # now contains 32 lists (a batch) of bbxs here we have to double access [0] in order to get the entry itself \n",
    "        # -> shape is (32, 1, 1, 7876)\n",
    "        bbx_pred = tf.reshape(y_pred, (batch_size, MAX_BBXS * 4))\n",
    "\n",
    "        losses = []\n",
    "        curr_true = []\n",
    "        curr_pred = []\n",
    "\n",
    "        unpacked_true = tf.unstack(bbx_true)\n",
    "        unpacked_pred = tf.unstack(bbx_pred)\n",
    "        for i in range(batch_size):\n",
    "            curr_true = unpacked_true[i] \n",
    "            curr_pred = unpacked_pred[i]\n",
    "\n",
    "\n",
    "            curr_true = tf.split(curr_true, MAX_BBXS)\n",
    "            curr_pred = tf.split(curr_pred, MAX_BBXS)\n",
    "\n",
    "            if len(curr_true) == 0:\n",
    "                curr_true.append(tf.concat([0., 0.,0.,0.]))\n",
    "\n",
    "            curr_loss = gl(curr_true, curr_pred)\n",
    "\n",
    "            losses.append(curr_loss)\n",
    "# tf.math.reduce_mean(losses, axis=-1)\n",
    "        return tf.math.real(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_true, y_pred):\n",
    "        # now contains 32 lists (a batch) of bbxs -> shape is (32, 7876)\n",
    "        bbx_true = y_true\n",
    "\n",
    "        # now contains 32 lists (a batch) of bbxs here we have to double access [0] in order to get the entry itself \n",
    "        # -> shape is (32, 1, 1, 7876)\n",
    "        bbx_pred = tf.reshape(y_pred, (1, 7876))\n",
    "\n",
    "        losses = []\n",
    "        curr_true = []\n",
    "        curr_pred = []\n",
    "\n",
    "        unpacked_true = tf.unstack(bbx_true)\n",
    "        unpacked_pred = tf.unstack(bbx_pred)\n",
    "        for i in range(1):\n",
    "            curr_true = unpacked_true[i] \n",
    "            curr_pred = unpacked_pred[i]\n",
    "\n",
    "\n",
    "            curr_true = tf.split(curr_true, 1969)\n",
    "            curr_pred = tf.split(curr_pred, 1969)\n",
    "\n",
    "            if len(curr_true) == 0:\n",
    "                curr_true.append(tf.concat([0., 0.,0.,0.]))\n",
    "\n",
    "            curr_loss = gl(curr_true, curr_pred)\n",
    "\n",
    "            losses.append(curr_loss)\n",
    "\n",
    "        return tf.math.reduce_mean(losses, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in tf.compat.v1.python_io.tf_record_iterator(OUTPUT_TRAIN_TFRECORD):\n",
    "    img, bbxs = parse_function(record)\n",
    "    bbxs = tf.reshape(bbxs, (1, 7876))\n",
    "    bbxs2 = copy.copy(bbxs)\n",
    "    bbxs2 = tf.reshape(bbxs2, (1, 1, 1, 7876))\n",
    "    x = loss_fn(bbxs, bbxs2)\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "opt = optimizers.Ftrl(learning_rate=1.0)\n",
    "# optimizer: Ftrl, learning_rate: 1.0, loss_fn: mean_squared_logarithmic_error, accuracy: [0.9937806129455566, 0.9937731027603149, 0.9937837719917297, 0.9937859773635864, 0.9937738180160522], loss: [0.07335256040096283, 0.07296431064605713, 0.07259127497673035, 0.07223349809646606, 0.07189014554023743], \n",
    "model.compile(optimizers.SGD(), loss=loss_fn_practical, metrics='accuracy', run_eagerly=True) #run_eagerly=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(train_dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=6)\n",
    "# history = model.fit(train_dataset, epochs=6, validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history['accuracy']\n",
    "model.history.history['loss'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(MODELS_PATH, \"face_recog_20211204_1323.h5\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "705bbc35638b6b0137416a4d11f2035b50e8f96623570785bfdd273baabb878a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('tf': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
