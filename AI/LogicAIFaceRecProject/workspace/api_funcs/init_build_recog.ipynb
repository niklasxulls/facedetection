{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import *\n",
    "import numpy as np\n",
    "import sys, os\n",
    "os.chdir('..')\n",
    "join = os.path.join(os.getcwd(), '_global')\n",
    "sys.path.extend([join])\n",
    "from workspace._global.config import *\n",
    "from workspace._global.funcs import *\n",
    "from workspace._global.bbx import *\n",
    "from workspace._global.recognition import *\n",
    "sys.path.extend([RESEARCH])\n",
    "from object_detection.utils import config_util\n",
    "import matplotlib.pyplot as plt\n",
    "from object_detection.builders import model_builder\n",
    "from IPython import get_ipython\n",
    "import cv2 \n",
    "import tensorflowjs as tfjs\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
    "ipython = get_ipython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_extract_faces(input_path, output_path, model):\n",
    "    if not os.path.exists(output_path):\n",
    "        os.mkdir(output_path)\n",
    "    imgs = [(f ,cv2.imread(os.path.join(input_path, f))) for f in os.listdir(input_path)]\n",
    "    for i in range(len(imgs)):\n",
    "        filename, img = imgs[i]\n",
    "        find_and_extract_face(model, img, os.path.join(output_path, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x19e0c6e96a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INIT_IMGS_1 = os.path.join(FACES_RECOG_IMG, \"INIT_FACES_1\")\n",
    "INIT_IMGS_2 = os.path.join(FACES_RECOG_IMG, \"INIT_FACES_2\")\n",
    "INIT_IMGS_CROPPED_1 = os.path.join(FACES_RECOG_IMG, \"INIT_FACES_CROPPED_1\")\n",
    "INIT_IMGS_CROPPED_2 = os.path.join(FACES_RECOG_IMG, \"INIT_FACES_CROPPED_2\")\n",
    "INIT_IMGS_TFRECORD_1 = os.path.join(FACE_RECOG_DATA, \"INIT_FACES_1.tfrecord\")\n",
    "INIT_IMGS_TFRECORD_2 = os.path.join(FACE_RECOG_DATA, \"INIT_FACES_2.tfrecord\")\n",
    "INIT_IMGS_1_LABEL_NAME = \"Init_Img_1\"\n",
    "INIT_IMGS_2_LABEL_NAME = \"Init_Img_2\"\n",
    "INIT_IMGS_1_LABEL_NUM = 1\n",
    "INIT_IMGS_2_LABEL_NUM = 2\n",
    "LAST_DENSE_BEFORE_OUTPUT_NEURONAS = 512\n",
    "\n",
    "config = config_util.get_configs_from_pipeline_file(PIPELINE_CONFIG_PATH)\n",
    "BBX_LOCALIZER = model_builder.build(model_config=config['model'], is_training=False)\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=BBX_LOCALIZER) \n",
    "ckpt.restore(os.path.join(MODEL, 'ckpt-12')).expect_partial()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_and_extract_faces(INIT_IMGS_1, INIT_IMGS_CROPPED_1, BBX_LOCALIZER)\n",
    "find_and_extract_faces(INIT_IMGS_2, INIT_IMGS_CROPPED_2, BBX_LOCALIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_images(INIT_IMGS_TFRECORD_1, INIT_IMGS_CROPPED_1, INIT_IMGS_1_LABEL_NAME, INIT_IMGS_1_LABEL_NUM, 3)\n",
    "parse_images(INIT_IMGS_TFRECORD_2, INIT_IMGS_CROPPED_2, INIT_IMGS_2_LABEL_NAME, INIT_IMGS_2_LABEL_NUM, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\xampp\\htdocs\\projects\\Face_Recognition_Ullsperger\\LogicAIFaceRecProject\\tf\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4526: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "INIT_DATASET = tf.data.TFRecordDataset([INIT_IMGS_TFRECORD_1,INIT_IMGS_TFRECORD_2])\n",
    "INIT_DATASET = INIT_DATASET.map(parse_fn)\n",
    "INIT_DATASET = INIT_DATASET.shuffle(330)\n",
    "INIT_DATASET = INIT_DATASET.batch(FACE_RECOG_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\niklas\\AppData\\Local\\Temp\\ipykernel_24592\\2081050494.py:1: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    }
   ],
   "source": [
    "for i in tf.compat.v1.python_io.tf_record_iterator(INIT_IMGS_TFRECORD_1):\n",
    "        img, label = parse_fn(i)\n",
    "        label = label.numpy()\n",
    "        num =  img.numpy()\n",
    "        plt.imsave(os.path.join(INSTALLATION_PATH, \"strcXXXYX\" + \".jpeg\"), img.numpy())\n",
    "\n",
    "        # plt.imshow(img)\n",
    "        # plt.show()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = applications.VGG19(\n",
    "    include_top=False,\n",
    "    input_shape=(*RECOG_IMAGE_SIZES, 3),\n",
    "    weights='imagenet'\n",
    ")\n",
    "base_model.trainable = False\n",
    "\n",
    "\n",
    "model = Sequential(base_model)\n",
    "model.add(layers.Conv2D(256, (4, 4), activation='tanh', padding=\"same\"))\n",
    "model.add(layers.MaxPool2D(strides=(2, 2), padding=\"same\"))\n",
    "model.add(layers.Conv2D(128, (4,4), activation='tanh', padding=\"same\"))\n",
    "model.add(layers.MaxPool2D(strides=(2, 2), padding=\"same\"))\n",
    "model.add(layers.Conv2D(256, (4,4), activation='tanh', padding=\"same\"))\n",
    "model.add(layers.MaxPool2D(strides=(2, 2), padding=\"same\"))\n",
    "model.add(layers.Conv2D(128, (4,4), activation='tanh', padding=\"same\"))\n",
    "model.add(layers.MaxPool2D(strides=(2, 2), padding=\"same\"))\n",
    "model.add(layers.Conv2D(512, (4,4), activation='tanh', padding=\"same\")) \n",
    "model.add(layers.MaxPool2D(strides=(2, 2), padding=\"same\"))\n",
    "model.add(layers.Conv2D(1024, (4,4), activation='tanh', padding=\"same\"))\n",
    "model.add(layers.MaxPool2D(strides=(2, 2), padding=\"same\"))\n",
    "model.add(layers.Conv2D(512, (4,4), activation='tanh', padding=\"same\"))\n",
    "model.add(layers.MaxPool2D(strides=(2, 2), padding=\"same\"))\n",
    "model.add(layers.Conv2D(1024, (4,4), activation='tanh', padding=\"same\"))\n",
    "model.add(layers.MaxPool2D(strides=(2, 2), padding=\"same\"))\n",
    "model.add(layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(512, activation='tanh'))\n",
    "model.add(tf.keras.layers.Dense(1024, activation='tanh'))\n",
    "model.add(tf.keras.layers.Dense(1024, activation='tanh'))\n",
    "model.add(tf.keras.layers.Dense(LAST_DENSE_BEFORE_OUTPUT_NEURONAS, activation='tanh'))\n",
    "model.add(layers.Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "78/78 [==============================] - 16s 174ms/step - loss: 0.9783 - accuracy: 0.5096\n",
      "Epoch 2/6\n",
      "78/78 [==============================] - 14s 174ms/step - loss: 0.7156 - accuracy: 0.5545A: 0s - loss: 0.7159 - accuracy: \n",
      "Epoch 3/6\n",
      "78/78 [==============================] - 14s 175ms/step - loss: 0.7137 - accuracy: 0.5096\n",
      "Epoch 4/6\n",
      "78/78 [==============================] - 14s 177ms/step - loss: 0.7058 - accuracy: 0.4712\n",
      "Epoch 5/6\n",
      "78/78 [==============================] - 14s 176ms/step - loss: 0.7025 - accuracy: 0.5288\n",
      "Epoch 6/6\n",
      "78/78 [==============================] - 14s 176ms/step - loss: 0.7037 - accuracy: 0.5224s - loss: 0.7043 -  -\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss=losses.CategoricalCrossentropy(from_logits=False), metrics='accuracy', run_eagerly=True) \n",
    "model.fit(INIT_DATASET, epochs=6)\n",
    "\n",
    "model.save(OUTPUT_FACES_RECOG)\n",
    "tfjs.converters.save_keras_model(model, FACES_RECOG_MODEL_EXPORT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(FACES_RECOG_LABEL_MAP)):\n",
    "    extend_label_map(1, 'unknown')\n",
    "    extend_label_map(2, 'init_img_1')\n",
    "    extend_label_map(3, 'init_img_2')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "24c78f2902c3eaca8402099206b187d2e0c79b256a5a127d981db2350636b3f9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('tf': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
